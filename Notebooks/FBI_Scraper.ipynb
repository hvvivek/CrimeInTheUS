{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data related to murder rates in the US is scraped from the FBI website. <br>The code below uses the UCR url at https://ucr.fbi.gov/ucr-publication and scrapes the data related to murders by Metrological Statistical Area (M.S.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import IFrame, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checks if words are part of any of the link titles a[title=?]\n",
    "def filter_links_title (links, included_words):\n",
    "    for link in links:\n",
    "        if link.get(\"title\"):\n",
    "            if(included_words in link.get(\"title\")):\n",
    "                return link.get(\"href\")\n",
    "\n",
    "# Checks if a certain marker is part of the table row\n",
    "def check_for_marker(els):\n",
    "    return_value = False\n",
    "    for el in els:\n",
    "        if el.get(\"rowspan\"):\n",
    "            if int(str(el.get(\"rowspan\"))) > 1:\n",
    "                return_value = True\n",
    "                break\n",
    "    return return_value\n",
    "\n",
    "# Returns true if a string is numeric\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Extracts population data from a row\n",
    "def get_pop(els):\n",
    "    for el in els:\n",
    "        if is_number(el.get_text().replace(\",\",\"\")):\n",
    "            return int(el.get_text().replace(\",\",\"\"))\n",
    "        \n",
    "# Imputes the data in a row using knn with 2 neighbors - Requires edits  \n",
    "def knn_impute_row(row, n):\n",
    "    row = row.copy(deep=True)\n",
    "    numbers = []\n",
    "    for i,col in enumerate(row):\n",
    "        if is_number(col):\n",
    "            if not np.isnan(col):\n",
    "                numbers.append(i)\n",
    "    for i,col in enumerate(row):\n",
    "        if(is_number(col)):\n",
    "            if(np.isnan(col)):\n",
    "                if(len(numbers) == 1):\n",
    "                    row[i] = row[numbers[0]]\n",
    "                else:\n",
    "                    distance = [np.abs(index - i) for index in numbers]\n",
    "                    knn_1 = row[numbers[np.argmin(distance)]]\n",
    "                    distance[np.argmin(distance)] = 9999\n",
    "                    knn_2 = row[numbers[np.argmin(distance)]]\n",
    "                    row[i] = (knn_1 + knn_2)/2\n",
    "    return row\n",
    "\n",
    "# Shorthand function to create axs of a certain size and ravel for easier graphing\n",
    "def get_axs(rows, columns, fig_size_width, fig_size_height):\n",
    "    dims = (fig_size_width, fig_size_height)\n",
    "    fig, axs = plt.subplots(rows, columns, figsize=dims)\n",
    "    if(rows*columns>1):\n",
    "         axs = axs.ravel()\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://ucr.fbi.gov/ucr-publications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req = requests.get(url)\n",
    "page = req.text\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = range(2006,2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_links = {}\n",
    "for year in years:\n",
    "    year_links[year] = soup.find_all(\"a\", string=year)[0][\"href\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract links for further scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldnt find link for year: 2009\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    url = year_links[year]\n",
    "    req = requests.get(url)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    if soup.find(\"a\", string = \"Violent Crime\"):\n",
    "        url = url.replace(\"/\" + url.split(\"/\")[-1], \"\")\n",
    "        link_add = str(soup.find(\"a\", string = \"Violent Crime\")[\"href\"])\n",
    "        if \"http\" not in link_add:\n",
    "            year_links[year] = url + \"/\" + link_add\n",
    "        else:\n",
    "            year_links[year] = link_add\n",
    "    else:\n",
    "        print(\"Couldnt find link for year: \" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link for 2009 manually entered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_links[2009] = \"https://www2.fbi.gov/ucr/cius2009/offenses/violent_crime/index.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigation to murder data by MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = year_links[year]\n",
    "    req = requests.get(url)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    link_add = str(soup.find(\"a\", string=\"Murder\")[\"href\"])\n",
    "    if link_add:\n",
    "       if \"http\" in link_add:\n",
    "           year_links[year] = link_add\n",
    "       else:\n",
    "           year_links[year] = url.replace(\"/\" + url.split(\"/\")[-1], \"\") + \"/\" +link_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = year_links[year]\n",
    "    req = requests.get(url)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    link_add = filter_links_title(soup.find_all(\"a\"), \"Metropolitan Statistical Area\")\n",
    "    if \"http\" in link_add:\n",
    "        year_links[year] = link_add\n",
    "    else:\n",
    "        n_times = link_add.count('../')\n",
    "        for i in range(0,n_times):\n",
    "            url = url.replace(\"/\" + url.split(\"/\")[-1], \"\")\n",
    "        year_links[year] = url.replace(\"/\" + url.split(\"/\")[-1], \"\") + \"/\" + link_add.replace(\"../\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSA_INDEX = 0\n",
    "POP_INDEX = 2\n",
    "NAME_INDEX = 0\n",
    "RATE_INDEX = 1\n",
    "DATA_INDEX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Downloads MSA name, population and murder rate\n",
    "murder_data = {}\n",
    "for year in years:\n",
    "    url = year_links[year]\n",
    "    req = requests.get(url)\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    x = soup.find(\"table\", \"data\")\n",
    "    rows = x.find(\"tbody\").find_all(\"tr\")\n",
    "    data = []\n",
    "    row_data = []\n",
    "    for row in rows:\n",
    "        elements = row.find_all([\"th\",\"td\"])\n",
    "        new_msa = check_for_marker(elements)\n",
    "        if new_msa:\n",
    "            if len(row_data)==2:\n",
    "                data.append(row_data)\n",
    "            row_data = []\n",
    "            text = str(elements[MSA_INDEX].get_text().split(\"M.S.A\")[0])\n",
    "            row_data.append(text.split(\",\")[0].strip()+\", \"+ text.split(\",\")[1].strip())\n",
    "            \n",
    "        if(\"Rate per 100,000 inhabitants\" in elements[NAME_INDEX].get_text()):\n",
    "            row_data.append(float(elements[DATA_INDEX].get_text()))\n",
    "    df = pd.DataFrame(data, columns=[\"msa\", str(year)]).sort_values(\"msa\", ascending=[0])\n",
    "    murder_data[year] = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining to create single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = murder_data[2006]\n",
    "years = range(2007,2017)\n",
    "for year in years:\n",
    "    result = pd.merge(result, murder_data[year], left_on='msa', right_on='msa', how='outer')\n",
    "years = range(2006,2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = result.sort_values([\"msa\"], ascending = [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting rows with missing data for imputation/manual fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_data = result[result.notnull().all(axis=1)]\n",
    "missing_data.to_csv(\"murder_data_missing.csv\")\n",
    "missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_missing_data = result[result.notnull().all(axis=1)]\n",
    "non_missing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataframe for final imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "murder_data = pd.read_csv(\"murder_data_full.csv\", index_col=0)\n",
    "murder_data.msa = murder_data.msa.str.replace(' M.D.','')\n",
    "murder_data.msa = murder_data.msa.str.replace('\\d+', '')\n",
    "murder_data.index = murder_data.msa\n",
    "murder_data.index = range(0, murder_data.shape[0])\n",
    "\n",
    "msa_name = [msa.split(',')[0].strip() for msa in murder_data.msa]\n",
    "msa_state = [msa.split(',')[1].strip() for msa in murder_data.msa]\n",
    "murder_data['msa_name'] = msa_name\n",
    "murder_data['msa_state'] = msa_state\n",
    "murder_data.msa_state = murder_data.msa_state.str.replace('-',',')\n",
    "murder_data.to_csv('FBI_Murder_Data_by_MSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rows = murder_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row_index in range(0, num_rows):\n",
    "    row = murder_data.iloc[row_index]\n",
    "    if murder_data.iloc[row_index].isnull().any():\n",
    "        imputed_row = knn_impute_row(row, 2)\n",
    "        murder_data.iloc[row_index] = imputed_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "murder_data.to_csv('FBI_Murder_Data_by_MSA_Imputed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
